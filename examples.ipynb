{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda processing allowed: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors: 100%|██████████| 4/4 [02:35<00:00, 38.78s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from jax import random\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "from lmkit.model import sampler, config as config_lib\n",
    "from lmkit.tools import compat\n",
    "\n",
    "repo = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_dir = \"models/llama3\"\n",
    "\n",
    "if not os.path.exists(model_dir) or not os.listdir(model_dir):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "    compat.from_hf(repo, model_dir, token=os.environ[\"HF_API_TOKEN\"])\n",
    "\n",
    "params = compat.params_to_lmkit(compat.gather_for_jax(model_dir))\n",
    "params = FrozenDict(params)\n",
    "config = compat.load_lmkit_config(f\"{model_dir}/config.json\")\n",
    "config = config_lib.extend_llama(config)\n",
    "\n",
    "tokenizer = compat.load_lmkit_tokenizer(\n",
    "    f\"{model_dir}/tokenizer.json\", f\"{model_dir}/generation_config.json\"\n",
    ")\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Question: What is a Josephson junction?\\nAnswer:\",\n",
    "    \"Question: What is the highest point of the Pamirs?\\nAnswer:\",\n",
    "]\n",
    "\n",
    "sampler.generate(\n",
    "    inputs=prompts,\n",
    "    max_new_tokens=1000,\n",
    "    tokenizer=tokenizer,\n",
    "    params=params,\n",
    "    config=config,\n",
    "    random_key=random.key(0),\n",
    "    return_text=True,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<grain._src.python.dataset.transformations.slice.SliceMapDataset at 0xf4fe7b5fc940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lmkit.tools import data\n",
    "\n",
    "datasource_file = \"data/shakespeare.txt\"\n",
    "with open(datasource_file, \"r\") as f:\n",
    "    text = f.read()\n",
    "data_iter = text.split(\"\\n\")\n",
    "\n",
    "batch_size=2048\n",
    "dataset_dir = \"data/dataset\"\n",
    "\n",
    "data.to_arrayrecord(\n",
    "    data_iter = data_iter,\n",
    "    out_dir=dataset_dir,\n",
    "    encode_fn=lambda x: x.encode(\"utf-8\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 11:27:05,397 - INFO - Starting BPE tokenizer training...\n",
      "2025-04-07 11:27:05,397 - INFO - Vocab size: 2048, Min frequency: 2\n",
      "2025-04-07 11:27:05,397 - INFO - Special tokens: ['<unk>', '<pad>', '<bos>', '<eos>']\n",
      "2025-04-07 11:27:05,398 - INFO - Saving to: trained_bpe_tokenizer.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 11:27:08,009 - INFO - Training complete.\n",
      "2025-04-07 11:27:08,017 - INFO - Tokenizer saved successfully to trained_bpe_tokenizer.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Testing the trained tokenizer ---\n",
      "Tokenizer loaded successfully from trained_bpe_tokenizer.json\n",
      "\n",
      "Original text: 'This is a test sentence.'\n",
      "Encoded IDs: [490, 112, 44, 57, 139, 1214, 204, 10]\n",
      "Tokens: ['this', 'Ġis', 'Ġa', 'Ġt', 'est', 'Ġsent', 'ence', '.']\n",
      "Decoded text: 'this is a test sentence.'\n",
      "\n",
      "Vocabulary size: 2048\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from typing import Optional, List, Iterator\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "def train_bpe(\n",
    "    iterator: Iterator[str],\n",
    "    vocab_size: int,\n",
    "    save_path: str,\n",
    "    min_frequency: int = 2,\n",
    "    special_tokens: Optional[List[str]] = None,\n",
    "    add_prefix_space: bool = False,\n",
    ") -> Tokenizer:\n",
    "    unk_token = \"<unk>\"\n",
    "    if special_tokens is None:\n",
    "        special_tokens = [unk_token, \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "    elif unk_token not in special_tokens:\n",
    "        # Add unk_token if user provided a list without it, as BPE model needs it\n",
    "        special_tokens = [unk_token] + special_tokens\n",
    "        logging.warning(f\"'{unk_token}' not found in special_tokens, adding it.\")\n",
    "\n",
    "    tokenizer = Tokenizer(BPE(unk_token=unk_token))\n",
    "\n",
    "    # 2. Setup Normalizer (optional but recommended)\n",
    "    # Normalizes text before tokenization (e.g., unicode, lowercase, accents)\n",
    "    tokenizer.normalizer = Sequence(\n",
    "        [\n",
    "            NFD(),  # Unicode normalization decomposes characters\n",
    "            Lowercase(),\n",
    "            StripAccents(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 3. Setup PreTokenizer (splits text into initial words/tokens)\n",
    "    # ByteLevel handles all bytes, good for diverse languages/data\n",
    "    tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=add_prefix_space)\n",
    "\n",
    "    # 4. Setup Decoder (to convert IDs back to text)\n",
    "    # Must match the pre_tokenizer\n",
    "    tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    # 5. Setup Trainer\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=special_tokens,\n",
    "        # initial_alphabet=pre_tokenizers.ByteLevel.alphabet(), # Use the ByteLevel alphabet\n",
    "        # Other options like `show_progress=True` can be added\n",
    "    )\n",
    "\n",
    "    # 6. Train the tokenizer\n",
    "    logging.info(f\"Starting BPE tokenizer training...\")\n",
    "    logging.info(f\"Vocab size: {vocab_size}, Min frequency: {min_frequency}\")\n",
    "    logging.info(f\"Special tokens: {special_tokens}\")\n",
    "    logging.info(f\"Saving to: {save_path}\")\n",
    "\n",
    "    tokenizer.train_from_iterator(iterator, trainer=trainer)\n",
    "\n",
    "    logging.info(\"Training complete.\")\n",
    "\n",
    "    # 7. Save the tokenizer\n",
    "    # Create directory if it doesn't exist\n",
    "    save_dir = os.path.dirname(save_path)\n",
    "    if save_dir and not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        logging.info(f\"Created directory: {save_dir}\")\n",
    "\n",
    "    tokenizer.save(save_path)\n",
    "    logging.info(f\"Tokenizer saved successfully to {save_path}\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer_dataset = data.grain_dataset_from(\n",
    "        arrayrecord_dir=dataset_dir,\n",
    "        batch_size=batch_size,\n",
    "        map_fn=lambda x: x.decode(\"utf-8\"),\n",
    "    )\n",
    "    data_iterator = iter(tokenizer_dataset)\n",
    "\n",
    "    # 2. Define parameters\n",
    "    VOCAB_SIZE = 2048  # Small vocab size for demo purposes\n",
    "    MIN_FREQ = 2\n",
    "    SAVE_PATH = \"trained_bpe_tokenizer.json\"\n",
    "\n",
    "    # 3. Train the tokenizer\n",
    "    trained_tokenizer = train_bpe_tokenizer(\n",
    "        iterator=data_iterator,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        save_path=SAVE_PATH,\n",
    "        min_frequency=MIN_FREQ,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FrozenDict({\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_tokens[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m: positions[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;66;03m# what if slicing gets into the picture\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_tokens[:, \u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m     35\u001b[0m     })\n\u001b[1;32m     38\u001b[0m loaded_dataset \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgrain_dataset_from(\n\u001b[1;32m     39\u001b[0m     arrayrecord_dir\u001b[38;5;241m=\u001b[39mdataset_dir,\n\u001b[1;32m     40\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     41\u001b[0m     map_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     42\u001b[0m     batch_map_fn\u001b[38;5;241m=\u001b[39mbatch_fn,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m final_params, final_opt_state \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     46\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m---> 47\u001b[0m     data_iterator\u001b[38;5;241m=\u001b[39m\u001b[43mdata_iter\u001b[49m,\n\u001b[1;32m     48\u001b[0m     num_steps\u001b[38;5;241m=\u001b[39mnum_steps,\n\u001b[1;32m     49\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m,\n\u001b[1;32m     50\u001b[0m     log_every\u001b[38;5;241m=\u001b[39mlog_granularity,\n\u001b[1;32m     51\u001b[0m     save_every\u001b[38;5;241m=\u001b[39msave_granularity,\n\u001b[1;32m     52\u001b[0m     checkpoint_dir\u001b[38;5;241m=\u001b[39mckpt_dir,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed. Final parameters and optimizer state returned.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_iter' is not defined"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "from lmkit.model import trainer\n",
    "from lmkit.tools import data\n",
    "\n",
    "config = FrozenDict({\n",
    "    \"num_layers\": 12,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_kv_heads\": 12,\n",
    "    \"hidden_size\": 768,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"act_fn\": \"silu\",\n",
    "    \"vocab_size\": ...,\n",
    "    \"max_position_embeddings\": 2048,\n",
    "    \"rope_base\": 100_000,\n",
    "    \"io_tying\": True,\n",
    "\n",
    "})\n",
    "\n",
    "batch_size = 4\n",
    "num_steps = 500  # Small number for demo\n",
    "log_granularity = 50\n",
    "save_granularity = 200\n",
    "ckpt_dir = \"checkpoints\"\n",
    "dataset_dir = \"data/dataset\"\n",
    "\n",
    "def batch_fn(x):\n",
    "    encoded = tokenizer.encode_batch_fast(x)\n",
    "    ids = [item.ids for item in encoded]\n",
    "    batch_tokens = jnp.asarray(ids).astype(jnp.int32)\n",
    "    positions = jnp.where(\n",
    "        batch_tokens != tokenizer.pad_token_id, jnp.arange(batch_tokens.shape[1]), 0\n",
    "    )\n",
    "    return FrozenDict({\n",
    "        \"input_ids\": batch_tokens[:, :-1],\n",
    "        \"positions\": positions[:, :-1], # what if slicing gets into the picture\n",
    "        \"target_ids\": batch_tokens[:, 1:],\n",
    "    })\n",
    "\n",
    "\n",
    "loaded_dataset = data.grain_dataset_from(\n",
    "    arrayrecord_dir=dataset_dir,\n",
    "    batch_size=batch_size,\n",
    "    map_fn=lambda x: x.decode(\"utf-8\"),\n",
    "    batch_map_fn=batch_fn,\n",
    ")\n",
    "\n",
    "final_params, final_opt_state = trainer.train(\n",
    "    config=config,\n",
    "    data_iterator=data_iter,\n",
    "    num_steps=num_steps,\n",
    "    learning_rate=5e-4,\n",
    "    log_every=log_granularity,\n",
    "    save_every=save_granularity,\n",
    "    checkpoint_dir=ckpt_dir,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed. Final parameters and optimizer state returned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
