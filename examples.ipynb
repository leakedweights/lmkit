{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed3088217724e8aabc161513259bb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"white-space: pre-wrap; font-family: monospace;\">&lt;|begin_of_text|&gt;User: Hello! What is a Josephson junction?\n",
       "Assistant: A Josephson junction is a device that consists of two superconducting materials separated by a thin insulating barrier. It is a fundamental component in the study of superconductivity and is used in various applications, including superconducting quantum interference devices (SQUIDs) and superconducting circuits.\n",
       "\n",
       "The Josephson junction was first predicted by Brian Josephson in 1962 and is based on the phenomenon of quantum tunneling, where electrons can tunnel through the insulating barrier between the two superconduct</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"white-space: pre-wrap; font-family: monospace;\">&lt;|begin_of_text|&gt;User: Say hello and then end the conversation.\n",
       "Assistant: Hello! It was nice chatting with you, but I&#x27;ll say goodbye now. Have a great day!  How can I help you today? \n",
       "(Note: The assistant is responding to the user&#x27;s initial greeting and then ending the conversation, as per the user&#x27;s request.) \n",
       "If you&#x27;d like to simulate a conversation, I can respond to your next message. Just let me know what&#x27;s on your mind!  Alternatively, you can also type &quot;goodbye&quot; to end the conversation immediately.&lt;pad&gt;</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from jax import random\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "from lmkit.model import sampler, config as config_lib\n",
    "from lmkit.tools import compat\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import html\n",
    "\n",
    "\n",
    "repo = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_dir = \"models/llama3\"\n",
    "\n",
    "if not os.path.exists(model_dir) or not os.listdir(model_dir):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "    compat.from_hf(repo, model_dir, token=os.environ[\"HF_API_TOKEN\"])\n",
    "\n",
    "params = compat.params_to_lmkit(compat.gather_for_jax(model_dir))\n",
    "params = FrozenDict(params)\n",
    "config = compat.load_lmkit_config(f\"{model_dir}/config.json\")\n",
    "config = config_lib.extend_llama(config)\n",
    "\n",
    "tokenizer = compat.to_lmkit_tokenizer(\n",
    "    f\"{model_dir}/tokenizer.json\", f\"{model_dir}/generation_config.json\"\n",
    ")\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"User: Hello! What is a Josephson junction?\\nAssistant:\",\n",
    "    \"User: Say hello and then end the conversation.\\nAssistant:\"\n",
    "]\n",
    "\n",
    "samples = sampler.generate(\n",
    "    inputs=prompts,\n",
    "    max_new_tokens=100,\n",
    "    tokenizer=tokenizer,\n",
    "    params=params,\n",
    "    config=config,\n",
    "    random_key=random.key(0),\n",
    "    return_text=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "for sample in tokenizer.decode_batch(samples, skip_special_tokens=False):\n",
    "    sanitized_sample = html.escape(sample)\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<div style=\"white-space: pre-wrap; font-family: monospace;\">{sanitized_sample}</div>'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models/demo\"\n",
    "tokenizer_path = f\"{model_dir}/tokenizer.json\"\n",
    "generation_config_path = f\"{model_dir}/generation_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Applied 'train' post-processor (BOS+EOS).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lmkit.tools import data, trainer, compat\n",
    "\n",
    "# 1. Extract dataset\n",
    "\n",
    "datasource_file = \"data/shakespeare.txt\"\n",
    "with open(datasource_file, \"r\") as f:\n",
    "    text = f.read()\n",
    "data_iter = text.split(\"\\n\")\n",
    "\n",
    "batch_size=2048\n",
    "dataset_dir = \"data/dataset\"\n",
    "\n",
    "if not os.path.exists(dataset_dir) or not os.listdir(dataset_dir):\n",
    "    data.to_arrayrecord(\n",
    "        data_iter = data_iter,\n",
    "        out_dir=dataset_dir,\n",
    "        encode_fn=lambda x: x.encode(\"utf-8\"),\n",
    "    )\n",
    "\n",
    "# 2. Train tokenizer\n",
    "vocab_size = 2048\n",
    "min_frequency = 2\n",
    "\n",
    "tokenizer_dataset = data.grain_dataset_from(\n",
    "    arrayrecord_dir=dataset_dir,\n",
    "    batch_size=batch_size,\n",
    "    map_fn=lambda x: x.decode(\"utf-8\"),\n",
    ")\n",
    "data_iterator = iter(tokenizer_dataset)\n",
    "\n",
    "tokenizer = trainer.train_tokenizer(\n",
    "    iterator=data_iterator,\n",
    "    vocab_size=vocab_size,\n",
    "    save_dir=model_dir,\n",
    "    generation_config={},\n",
    "    min_frequency=min_frequency,\n",
    ")\n",
    "\n",
    "tokenizer = compat.load_tokenizer(\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    mode=\"train\",\n",
    "    generation_config_file=generation_config_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621c9a6584fa4599a8caaa168515895c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed. Final parameters and optimizer state returned.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.core import FrozenDict\n",
    "import logging\n",
    "from lmkit.tools import data\n",
    "\n",
    "jax.config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "config = FrozenDict({\n",
    "    \"num_layers\": 12,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_kv_heads\": 12,\n",
    "    \"hidden_size\": 768,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"act_fn\": jax.nn.silu,\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"max_position_embeddings\": 2048,\n",
    "    \"rope_base\": 100_000,\n",
    "    \"norm_eps\": 1e-6,\n",
    "    \"io_tying\": True,\n",
    "\n",
    "})\n",
    "\n",
    "batch_size = 2048\n",
    "num_steps = 500\n",
    "log_granularity = 50\n",
    "save_granularity = 200\n",
    "ckpt_dir = \"checkpoints\"\n",
    "dataset_dir = \"data/dataset\"\n",
    "\n",
    "def batch_map_fn(batch_text, min_final_len: int = 2):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        raise ValueError(\"Tokenizer must have pad_token_id set for padding.\")\n",
    "\n",
    "    encoded = tokenizer.encode_batch_fast(batch_text)\n",
    "    if not encoded:\n",
    "        return None\n",
    "\n",
    "    logging.info(\"Decoded: {tokenizer.decode_batch(encoded, skip_special_tokens=False)}\")\n",
    "\n",
    "    max_len = max(len(item.ids) for item in encoded) if encoded else 0\n",
    "    if max_len == 0:\n",
    "        logging.debug(\"Skipping batch: max sequence length after tokenization is 0.\")\n",
    "        return None\n",
    "\n",
    "    ids = [\n",
    "        item.ids + [tokenizer.pad_token_id] * (max_len - len(item.ids))\n",
    "        for item in encoded\n",
    "    ]\n",
    "    initial_batch_tokens = jnp.array(ids, dtype=jnp.int32)\n",
    "\n",
    "    current_len = initial_batch_tokens.shape[1]\n",
    "    pad_amount = 1 - (current_len % 2)\n",
    "    paddings = ((0, 0), (0, pad_amount))\n",
    "\n",
    "    padded_tokens_for_slicing = jnp.pad(\n",
    "        initial_batch_tokens,\n",
    "        paddings,\n",
    "        mode=\"constant\",\n",
    "        constant_values=tokenizer.pad_token_id,\n",
    "    )\n",
    "    odd_len = padded_tokens_for_slicing.shape[1]\n",
    "\n",
    "    final_len = odd_len - 1\n",
    "    if final_len < min_final_len:\n",
    "        logging.debug(\n",
    "            f\"Skipping batch: final sequence length ({final_len}) < min_final_len ({min_final_len}).\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    positions_for_slicing = jnp.where(\n",
    "        padded_tokens_for_slicing != tokenizer.pad_token_id, jnp.arange(odd_len), -1\n",
    "    )\n",
    "\n",
    "    input_ids = padded_tokens_for_slicing[:, :-1]\n",
    "    input_positions = positions_for_slicing[:, :-1]\n",
    "    target_ids = padded_tokens_for_slicing[:, 1:]\n",
    "\n",
    "    return FrozenDict(\n",
    "        {\n",
    "            \"input_ids\": input_ids.astype(jnp.int32),\n",
    "            \"positions\": input_positions.astype(jnp.int32),\n",
    "            \"target_ids\": target_ids.astype(jnp.int32),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "dataset = data.grain_dataset_from(\n",
    "    arrayrecord_dir=dataset_dir,\n",
    "    batch_size=batch_size,\n",
    "    map_fn=lambda x: x.decode(\"utf-8\"),\n",
    "    batch_map_fn=batch_map_fn,\n",
    ")\n",
    "dataset = dataset.repeat(50)\n",
    "\n",
    "final_params, final_opt_state = trainer.train_model(\n",
    "    config=config,\n",
    "    data_iterator=iter(dataset.to_iter_dataset()),\n",
    "    num_steps=num_steps,\n",
    "    learning_rate=1e-2,\n",
    "    log_every=log_granularity,\n",
    "    save_every=save_granularity,\n",
    "    checkpoint_dir=ckpt_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 'inference' post-processor (BOS only).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef221e6fe2d44e38b3ceb2b7fdf98c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>first citizen:<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "from lmkit.model import sampler\n",
    "from jax import random\n",
    "\n",
    "sampling_tokenizer = compat.load_tokenizer(\n",
    "    tokenizer_path, mode=\"inference\", generation_config_file=generation_config_path\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"First\"\n",
    "]\n",
    "\n",
    "generated = sampler.generate(\n",
    "    inputs=prompts,\n",
    "    max_new_tokens=100,\n",
    "    tokenizer=sampling_tokenizer,\n",
    "    params=final_params,\n",
    "    config=config,\n",
    "    random_key=random.key(0),\n",
    "    return_text=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(sampling_tokenizer.decode(generated[0], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
