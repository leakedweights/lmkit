{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from lmkit.model import transformer, config as config_lib\n",
    "from lmkit.tools import compat\n",
    "\n",
    "repo = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_dir = \"models/llama3\"\n",
    "\n",
    "if not os.path.exists(model_dir) or not os.listdir(model_dir):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "    compat.from_hf(repo, model_dir, token=os.environ[\"HF_API_TOKEN\"])\n",
    "\n",
    "params = compat.params_to_lmkit(compat.gather_for_jax(model_dir))\n",
    "config = compat.load_lmkit_config(f\"{model_dir}/config.json\")\n",
    "config = config_lib.extend_llama(config)\n",
    "\n",
    "tokenizer = compat.load_lmkit_tokenizer(\n",
    "    f\"{model_dir}/tokenizer.json\", f\"{model_dir}/generation_config.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "(128256,)\n",
      "25\n",
      "(128256,)\n",
      "13\n",
      "(128256,)\n"
     ]
    }
   ],
   "source": [
    "for seq_logits, last_idx in zip(output_logits, seq_lens-1):\n",
    "    print(last_idx)\n",
    "    print(seq_logits[last_idx].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = [seq_logits[last_idx] for seq_logits, last_idx in zip(output_logits, seq_lens-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='white-space: pre-wrap;'>Prompt: Question: Please give me a haiku about writing JAX code!\n",
       "Completion:  \n",
       "Answer: \n",
       "Syntax rules my fate\n",
       "Code flows like a river's stream\n",
       "Logic's gentle art\n",
       "\n",
       "Note: JAX is a Java framework for building web applications. The haiku is a poetic representation of the experience of writing JAX code, emphasizing the importance of syntax, the flow of code, and the art of logic. \n",
       "\n",
       "Here's a breakdown of the haiku:\n",
       "\n",
       "* \"Syntax rules my fate\" - This line highlights the importance of following the</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Step 94, seq_lens: [109]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "\n",
    "# --- Initial Setup ---\n",
    "initial_texts = [\n",
    "    \"Question: Please give me a haiku about writing JAX code!\",\n",
    "    \"Question: Please give me an elaborate python code to train a transformer with pytorch.\",\n",
    "    \"Question: Capital of France!\",\n",
    "]\n",
    "\n",
    "encoded_batch = tokenizer.encode_batch(initial_texts)\n",
    "model_inputs = jnp.array(list(map(lambda x: x.ids, encoded_batch)), dtype=jnp.int32)\n",
    "\n",
    "# --- Calculate Initial Positions and Sequence Lengths ---\n",
    "# Create position IDs: 0, 1, 2, ... for non-pad tokens, -1 for pad tokens\n",
    "positions = jnp.where(\n",
    "    model_inputs != tokenizer.pad_token_id,\n",
    "    jnp.arange(model_inputs.shape[1], dtype=jnp.int32),  # Assign index 0, 1, 2...\n",
    "    -1,  # Assign -1 for padding (or choose another indicator if your model expects something else)\n",
    ")\n",
    "\n",
    "seq_lens = jnp.sum(model_inputs != tokenizer.pad_token_id, axis=1, dtype=jnp.int32)\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "batch_size = model_inputs.shape[0]\n",
    "batch_indices = jnp.arange(batch_size)\n",
    "max_new_tokens = 100\n",
    "temperature = 0.3\n",
    "key = jax.random.key(2002)\n",
    "\n",
    "# --- Tracking ---\n",
    "completed = [False for _ in range(batch_size)]\n",
    "# Decode initial prompts correctly, handling padding\n",
    "current_outputs = []\n",
    "for i in range(batch_size):\n",
    "    prompt_tokens = model_inputs[i][: seq_lens[i]]  # Get only non-pad tokens\n",
    "    prompt_text = tokenizer.decode(prompt_tokens.tolist())  # Decode the actual prompt\n",
    "    current_outputs.append(\n",
    "        f\"Prompt: {initial_texts[i]}\\nCompletion: \"\n",
    "    )  # Use original text for clarity\n",
    "\n",
    "\n",
    "# --- Initial Display ---\n",
    "clear_output(wait=True)\n",
    "for output in current_outputs:\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap;'>{output}</div><hr>\"))\n",
    "\n",
    "\n",
    "# --- Generation Loop ---\n",
    "for i in range(max_new_tokens):\n",
    "    print(f\"DEBUG: Step {i}, seq_lens: {seq_lens}\")  # Optional debug print\n",
    "\n",
    "    # --- Model Forward Pass ---\n",
    "    output_logits = transformer.run_decoder(model_inputs, positions, params, config)\n",
    "\n",
    "    # --- Select Logits for Next Token ---\n",
    "    print(f\"Indices: {seq_lens - 1}\")\n",
    "    logits = jnp.array([\n",
    "        seq_logits[last_idx]\n",
    "        for seq_logits, last_idx in zip(output_logits, seq_lens - 1)\n",
    "    ])\n",
    "\n",
    "    # --- Sample Next Token ---\n",
    "    scaled_logits = logits / jnp.maximum(temperature, 1e-6)  # Add epsilon for safety\n",
    "    step_key = jax.random.fold_in(key, i)\n",
    "    # Ensure next_tokens are int32\n",
    "    next_tokens = jax.random.categorical(step_key, scaled_logits, axis=-1).astype(\n",
    "        jnp.int32\n",
    "    )\n",
    "    # Add sequence dimension: shape (batch_size,) -> (batch_size, 1)\n",
    "    next_tokens_expanded = next_tokens[:, None]\n",
    "    print(f\"DEBUG: Step {i}, sampled tokens: {next_tokens}\")  # Optional debug print\n",
    "\n",
    "    # break\n",
    "\n",
    "    # --- Update State ---\n",
    "    # Check if we need to expand the arrays (common in simpler loops, might not be needed with KV caching)\n",
    "    current_max_len = model_inputs.shape[1]\n",
    "    if jnp.any(seq_lens >= current_max_len):\n",
    "        # Expand arrays by one position if any sequence hits the current max length\n",
    "        print(\n",
    "            f\"DEBUG: Step {i}, Expanding arrays from {current_max_len}\"\n",
    "        )  # Optional debug print\n",
    "        # Pad model_inputs with PAD_TOKEN_ID\n",
    "        model_inputs = jnp.concatenate(\n",
    "            [\n",
    "                model_inputs,\n",
    "                jnp.full((batch_size, 1), tokenizer.pad_token_id, dtype=jnp.int32),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        # Pad positions with -1 (or your chosen padding indicator)\n",
    "        positions = jnp.concatenate(\n",
    "            [positions, jnp.full((batch_size, 1), -1, dtype=jnp.int32)], axis=1\n",
    "        )\n",
    "\n",
    "    # Indices to update: (batch_idx, sequence_idx) where sequence_idx is the current length\n",
    "    update_indices = (batch_indices, seq_lens)\n",
    "    model_inputs = model_inputs.at[update_indices].set(next_tokens)\n",
    "\n",
    "    positions = positions.at[update_indices].set(seq_lens)\n",
    "\n",
    "    seq_lens += 1\n",
    "\n",
    "    # --- Decode and Check Completion ---\n",
    "    next_tokens_list = next_tokens.tolist()  # Use the original (B,) shape tokens\n",
    "    all_sequences_completed = True  # Assume all complete until proven otherwise\n",
    "    for idx, token_id in enumerate(next_tokens_list):\n",
    "        if not completed[idx]:\n",
    "            token_str = tokenizer.decode(\n",
    "                [token_id], skip_special_tokens=False\n",
    "            )  # Decode single token\n",
    "            current_outputs[idx] += token_str  # Append decoded string\n",
    "\n",
    "            if token_id == tokenizer.eos_token_id:\n",
    "                completed[idx] = True\n",
    "                print(\n",
    "                    f\"DEBUG: Step {i}, Sequence {idx} completed (EOS).\"\n",
    "                )  # Optional debug print\n",
    "            else:\n",
    "                all_sequences_completed = (\n",
    "                    False  # At least one sequence is still running\n",
    "                )\n",
    "        else:\n",
    "            # Keep track if all were *already* complete\n",
    "            pass\n",
    "\n",
    "    # --- Display Update ---\n",
    "    clear_output(wait=True)\n",
    "    for output_idx, output in enumerate(current_outputs):\n",
    "        completion_marker = \" [COMPLETED]\" if completed[output_idx] else \"\"\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"<div style='white-space: pre-wrap;'>{output}{completion_marker}</div><hr>\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # --- Check for Early Exit ---\n",
    "    if all_sequences_completed or all(\n",
    "        completed\n",
    "    ):  # Check if all are newly or previously completed\n",
    "        print(f\"DEBUG: All sequences completed at step {i}.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- Generation Finished ---\")\n",
    "# Final outputs are already stored in current_outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
