{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda processing allowed: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors: 100%|██████████| 4/4 [02:44<00:00, 41.18s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from lmkit.model import transformer, config as config_lib\n",
    "from lmkit.tools import compat\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "repo = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_dir = \"models/llama3\"\n",
    "\n",
    "if not os.path.exists(model_dir) or not os.listdir(model_dir):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "    compat.from_hf(repo, model_dir, token=os.environ[\"HF_API_TOKEN\"])\n",
    "\n",
    "params = compat.params_to_lmkit(compat.gather_for_jax(model_dir))\n",
    "params = FrozenDict(params)\n",
    "config = compat.load_lmkit_config(f\"{model_dir}/config.json\")\n",
    "config = config_lib.extend_llama(config)\n",
    "\n",
    "tokenizer = compat.load_lmkit_tokenizer(\n",
    "    f\"{model_dir}/tokenizer.json\", f\"{model_dir}/generation_config.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "\n",
    "def rms_norm(x, weight, eps=1e-6):\n",
    "    orig_dtype = x.dtype\n",
    "    x = x.astype(jnp.float32)\n",
    "    normed = x * jax.lax.rsqrt(jnp.mean(x**2, axis=-1, keepdims=True) + eps)\n",
    "    out = weight * normed.astype(orig_dtype)\n",
    "    return out\n",
    "\n",
    "def ffn(x, params, act_fn):\n",
    "    gate = x @ params[\"W_gate\"]\n",
    "    act = act_fn(gate)\n",
    "    up = x @ params[\"W_up\"]\n",
    "    output = (act * up) @ params[\"W_down\"]\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_rope(positions, head_dim, base):\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(f\"head_dim must be even, got {head_dim}\")\n",
    "\n",
    "    inv_freq = 1.0 / (\n",
    "        base ** (jnp.arange(0, head_dim, 2, dtype=jnp.float32) / head_dim)\n",
    "    )\n",
    "    positions = positions.astype(jnp.float32)\n",
    "    freqs = positions[:, :, None] * inv_freq[None, None, :]\n",
    "    emb = jnp.concatenate((freqs, freqs), axis=-1)\n",
    "\n",
    "    pad_mask = positions >= 0\n",
    "    pad_mask = pad_mask[:, :, None].repeat(head_dim, axis=-1)\n",
    "    emb = jnp.where(pad_mask, emb, 0.0)\n",
    "\n",
    "    sin_values = jnp.sin(emb)\n",
    "    cos_values = jnp.cos(emb)\n",
    "\n",
    "    return sin_values, cos_values\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return jnp.concatenate((-x2, x1), axis=-1)\n",
    "\n",
    "\n",
    "def rope(x, sin, cos):\n",
    "    if x.ndim == 4 and sin.ndim == 3:\n",
    "        sin = sin[:, :, None, :]\n",
    "        cos = cos[:, :, None, :]\n",
    "    elif x.ndim > sin.ndim and x.shape[-1] == sin.shape[-1]:\n",
    "        num_broadcast_dims = x.ndim - sin.ndim\n",
    "        new_shape = list(sin.shape)\n",
    "        for _ in range(num_broadcast_dims):\n",
    "            new_shape.insert(-1, 1) \n",
    "        sin = jnp.reshape(sin, new_shape)\n",
    "        cos = jnp.reshape(cos, new_shape)\n",
    "        if sin.shape[:-1] != x.shape[:-1] or cos.shape[:-1] != x.shape[:-1]:\n",
    "            try:\n",
    "                sin = sin[..., None, :]\n",
    "                cos = cos[..., None, :]\n",
    "            except IndexError:\n",
    "                raise ValueError(\n",
    "                    f\"Cannot broadcast sin/cos shapes {sin.shape} to x shape {x.shape}\"\n",
    "                )\n",
    "\n",
    "    rotated_x = (x * cos) + (rotate_half(x) * sin)\n",
    "    return rotated_x.astype(x.dtype)\n",
    "\n",
    "@partial(jax.vmap, in_axes=(0, 0, 0, 0, 0))\n",
    "def update_2d(arr1, arr2, update1, update2, start_idx):\n",
    "    arr1_update = jax.lax.dynamic_update_slice_in_dim(arr1, update1, start_idx, axis=0)\n",
    "    arr2_update = jax.lax.dynamic_update_slice_in_dim(arr2, update2, start_idx, axis=0)\n",
    "    return arr1_update, arr2_update\n",
    "\n",
    "def attention(inputs, cache, params, config):\n",
    "    positions = cache.positions\n",
    "    seq_lens = jnp.max(positions, axis=-1).astype(jnp.int32) + 1\n",
    "\n",
    "    sin, cos = cache.sin, cache.cos\n",
    "\n",
    "    query = inputs @ params[\"W_q\"]\n",
    "    key = inputs @ params[\"W_k\"]\n",
    "    value = inputs @ params[\"W_v\"]\n",
    "    \n",
    "    query = rearrange(query, \"... t (n h) -> ... t n h\", n=config[\"num_heads\"])\n",
    "    query = rope(query, sin, cos)\n",
    "    key = rearrange(key, \"... t (n h) -> ... t n h\", n=config[\"num_kv_heads\"])\n",
    "    key = rope(key, sin, cos)\n",
    "    value = rearrange(value, \"... t (n h) -> ... t n h\", n=config[\"num_kv_heads\"])\n",
    "\n",
    "\n",
    "    full_key, full_value = key, value\n",
    "    query_seq_lens = seq_lens\n",
    "    if cache.keys is not None:\n",
    "        full_key, full_value = update_2d(cache.keys, cache.values, key, value, cache.cached_lens)\n",
    "        # query_seq_lens = jnp.ones((inputs.shape[0],)).astype(jnp.int32)\n",
    "\n",
    "    x = jax.nn.dot_product_attention(\n",
    "        query=query,\n",
    "        key=full_key,\n",
    "        value=full_value,\n",
    "        is_causal=cache.keys is None,\n",
    "        query_seq_lengths=query_seq_lens,\n",
    "        key_value_seq_lengths=seq_lens,\n",
    "        implementation=\"cudnn\",\n",
    "    )\n",
    "\n",
    "    x = rearrange(x, \"... t n h -> ... t (n h)\")\n",
    "    x = x @ params[\"W_o\"]\n",
    "\n",
    "    return x, cache.replace(keys=full_key, values=full_value)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3,))\n",
    "def run(inputs, cache, params, config):\n",
    "    x = jnp.take(params[\"embed_table\"], inputs, axis=0, fill_value=-1e6)\n",
    "\n",
    "    new_layer_cache = []\n",
    "\n",
    "    for i, layer_params in enumerate(params[\"layers\"]):\n",
    "        y = rms_norm(x, layer_params[\"input_norm\"], eps=config[\"norm_eps\"])\n",
    "        attn_out, layer_cache = attention(y, cache.layers[i], layer_params[\"attn\"], config)\n",
    "        new_layer_cache.append(layer_cache)\n",
    "\n",
    "        x = x + attn_out\n",
    "        y = rms_norm(x, layer_params[\"post_attn_norm\"], eps=config[\"norm_eps\"])\n",
    "        ffn_out = ffn(y, layer_params[\"ffn\"], config[\"act_fn\"])\n",
    "        x = x + ffn_out\n",
    "\n",
    "    x = rms_norm(x, params[\"out_norm\"], eps=config[\"norm_eps\"])\n",
    "    logits = x @ params[\"lm_head\"]\n",
    "\n",
    "    if cache.use_kv:\n",
    "        cache = cache.replace(layers=new_layer_cache)\n",
    "    return logits, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bd7c0da3314ff98fe548da82923b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from flax import struct\n",
    "from typing import Optional, List\n",
    "\n",
    "@struct.dataclass\n",
    "class LayerCache:\n",
    "    sin: jnp.array\n",
    "    cos: jnp.array\n",
    "    cached_lens: jnp.array \n",
    "    positions: jnp.array\n",
    "    keys: Optional[jnp.array] = None\n",
    "    values: Optional[jnp.array] = None\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TransformerCache:\n",
    "    use_kv: bool = struct.field(pytree_node=False)\n",
    "    layers: List[LayerCache]\n",
    "    full_positions: jnp.array\n",
    "    full_sin: jnp.array\n",
    "    full_cos: jnp.array\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(\n",
    "        cls, batch_size, current_positions, config, max_total_length=0, use_kv=False\n",
    "    ):\n",
    "\n",
    "        head_dim = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "        positions = jnp.arange(max_total_length).astype(jnp.int32)\n",
    "        positions = jnp.broadcast_to(positions, (batch_size, max_total_length))\n",
    "        sin, cos = build_rope(positions, head_dim, config[\"rope_base\"])\n",
    "            \n",
    "        layers = [\n",
    "            LayerCache(\n",
    "                sin=sin,\n",
    "                cos=cos,\n",
    "                cached_lens=jnp.zeros((batch_size,)).astype(jnp.int32),\n",
    "                positions=current_positions,\n",
    "                keys=None,\n",
    "                values=None,\n",
    "            )\n",
    "            for _ in range(config[\"num_layers\"])\n",
    "        ]\n",
    "        return cls(layers=layers, use_kv=use_kv, full_sin=sin, full_cos=cos, full_positions=positions)\n",
    "\n",
    "    def roll(self):\n",
    "        batch_indices = jnp.arange(self.full_positions.shape[0]).astype(jnp.int32)\n",
    "        first_layer = self.layers[0]\n",
    "        seq_lens = jnp.max(first_layer.positions, axis=-1).astype(jnp.int32) + 1\n",
    "\n",
    "        full_positions = expand_and_set(self.full_positions, seq_lens, seq_lens, fill=-1)\n",
    "\n",
    "        if self.use_kv:\n",
    "            cached_lens = seq_lens\n",
    "            new_positions = full_positions[batch_indices, seq_lens][..., None]\n",
    "            new_sin = self.full_sin[batch_indices, seq_lens][:, None, :]\n",
    "            new_cos = self.full_cos[batch_indices, seq_lens][:, None, :]\n",
    "        else:\n",
    "            cached_lens = first_layer.cached_lens\n",
    "            new_positions = full_positions\n",
    "            new_sin = self.full_sin\n",
    "            new_cos = self.full_cos\n",
    "\n",
    "        new_layers = []\n",
    "        for layer in self.layers:\n",
    "            new_layers.append(layer.replace(positions=new_positions,\n",
    "                cached_lens=cached_lens, sin=new_sin, cos=new_cos))\n",
    "                        \n",
    "        return self.replace(layers=new_layers, full_positions=full_positions)\n",
    "\n",
    "\n",
    "def expand_and_set(arr, indices, values, fill):\n",
    "    while jnp.max(indices) >= arr.shape[-1]:\n",
    "        filler = fill * jnp.ones((*arr.shape[:-1], 1))\n",
    "        arr = jnp.concatenate([arr, filler], axis=-1).astype(arr.dtype)\n",
    "    arr = arr.at[jnp.arange(arr.shape[0]), indices].set(values)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def generate(inputs, max_new_tokens, tokenizer, params, config):\n",
    "    batch_size = len(inputs)\n",
    "\n",
    "    encodings = tokenizer.encode_batch_fast(inputs)\n",
    "    tokens = jnp.array([enc.ids for enc in encodings])\n",
    "    tokens = jnp.concatenate([tokens, tokenizer.pad_token_id * jnp.ones((batch_size, max_new_tokens))], axis=-1).astype(jnp.int32)\n",
    "    positions = jnp.where(tokens != tokenizer.pad_token_id, jnp.arange(tokens.shape[-1]), -1)\n",
    "    seq_lens = jnp.sum(positions >= 0, axis=-1)\n",
    "\n",
    "    model_inputs = tokens\n",
    "\n",
    "    cache = TransformerCache.initialize(\n",
    "        batch_size=batch_size,\n",
    "        current_positions=positions,\n",
    "        config=config,\n",
    "        max_total_length=jnp.max(seq_lens + max_new_tokens),\n",
    "        use_kv=True,\n",
    "    )\n",
    "\n",
    "    for _ in tqdm(range(max_new_tokens)):\n",
    "        logits, cache = run(model_inputs, cache, params, config)\n",
    "        next_token_logits = logits[jnp.arange(batch_size), seq_lens-1, :]\n",
    "        next_tokens = jnp.argmax(next_token_logits, axis=-1)        \n",
    "        tokens = expand_and_set(tokens, seq_lens, next_tokens, fill=-1)\n",
    "        model_inputs = next_tokens[..., None] if cache.use_kv else tokens\n",
    "        cache = cache.roll()\n",
    "        seq_lens += 1\n",
    "\n",
    "    return tokenizer.decode_batch(\n",
    "        jnp.where(tokens >= 0, tokens, tokenizer.pad_token_id)\n",
    "    )\n",
    "\n",
    "prompts = [\n",
    "    \"What is a Josephson junction?\",\n",
    "]\n",
    "\n",
    "generate(prompts, 10000, tokenizer, params, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache shape: (2, 128, 32, 64)\n",
      "updates shape: (2, 1, 32, 64)\n",
      "start_indices shape: (2,)\n",
      "\n",
      "updated_cache1 shape: (2, 128, 32, 64)\n",
      "updated_cache2 shape: (2, 128, 32, 64)\n",
      "\n",
      "Value at index [0, 0, 0, 0] (should be 0): 0.0\n",
      "Value at index [0, 1, 0, 0] (should be 1): 1.0\n",
      "Value at index [0, 2, 0, 0] (should be 0): 0.0\n",
      "\n",
      "Value at index [1, 0, 0, 0] (should be 0): 0.0\n",
      "Value at index [1, 1, 0, 0] (should be 1): 1.0\n",
      "Value at index [1, 2, 0, 0] (should be 0): 0.0\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Corrected in_axes: change the last element from None to 0\n",
    "@partial(jax.vmap, in_axes=(0, 0, 0, 0, 0))\n",
    "def update_2d(arr1, arr2, update1, update2, start_idx):\n",
    "    # Inside the vmapped function for a single batch element:\n",
    "    # arr1, arr2 shape: (seq_len, num_heads, head_dim) -> e.g., (128, 32, 64)\n",
    "    # update1, update2 shape: (1, num_heads, head_dim) -> e.g., (1, 32, 64)\n",
    "    # start_idx: integer -> e.g., 1\n",
    "    arr1_update = jax.lax.dynamic_update_slice_in_dim(arr1, update1, start_idx, axis=0)\n",
    "    arr2_update = jax.lax.dynamic_update_slice_in_dim(arr2, update2, start_idx, axis=0)\n",
    "    return arr1_update, arr2_update\n",
    "\n",
    "\n",
    "# Batch size is 2\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "num_heads = 32\n",
    "head_dim = 64\n",
    "\n",
    "cache = jnp.zeros((batch_size, seq_len, num_heads, head_dim))\n",
    "updates = jnp.ones((batch_size, 1, num_heads, head_dim))\n",
    "\n",
    "# Provide one start index per batch element\n",
    "# In this case, both are 1, but they could be different, e.g., jnp.array([1, 5])\n",
    "start_indices = jnp.array([1, 1])\n",
    "\n",
    "# Check shapes\n",
    "print(\"cache shape:\", cache.shape)\n",
    "print(\"updates shape:\", updates.shape)\n",
    "print(\"start_indices shape:\", start_indices.shape)\n",
    "\n",
    "# Call the function\n",
    "updated_cache1, updated_cache2 = update_2d(\n",
    "    cache, cache, updates, updates, start_indices\n",
    ")\n",
    "\n",
    "# Check output shapes and content\n",
    "print(\"\\nupdated_cache1 shape:\", updated_cache1.shape)\n",
    "print(\"updated_cache2 shape:\", updated_cache2.shape)\n",
    "\n",
    "# Verify the update (optional)\n",
    "print(\"\\nValue at index [0, 0, 0, 0] (should be 0):\", updated_cache1[0, 0, 0, 0])\n",
    "print(\"Value at index [0, 1, 0, 0] (should be 1):\", updated_cache1[0, 1, 0, 0])\n",
    "print(\"Value at index [0, 2, 0, 0] (should be 0):\", updated_cache1[0, 2, 0, 0])\n",
    "\n",
    "print(\"\\nValue at index [1, 0, 0, 0] (should be 0):\", updated_cache1[1, 0, 0, 0])\n",
    "print(\"Value at index [1, 1, 0, 0] (should be 1):\", updated_cache1[1, 1, 0, 0])\n",
    "print(\"Value at index [1, 2, 0, 0] (should be 0):\", updated_cache1[1, 2, 0, 0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
