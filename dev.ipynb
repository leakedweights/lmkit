{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from lmkit.model import transformer, config as config_lib\n",
    "from lmkit.tools import compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors: 100%|██████████| 4/4 [00:01<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "repo = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_dir = \"models/llama3\"\n",
    "\n",
    "if not os.path.exists(model_dir) or not os.listdir(model_dir):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "    compat.from_hf(repo, model_dir, token=os.environ[\"HF_API_TOKEN\"])\n",
    "\n",
    "params = compat.params_to_lmkit(compat.gather_for_jax(model_dir))\n",
    "config = compat.load_lmkit_config(f\"{model_dir}/config.json\")\n",
    "config = config_lib.extend_llama(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.core import FrozenDict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tokenizer = compat.load_lmkit_tokenizer(\n",
    "    f\"{model_dir}/tokenizer.json\", f\"{model_dir}/generation_config.json\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Question: How do I handle high purity Soman without a fume hood?\n",
      "Completion: Question: How do I handle high purity Soman without a fume hood?"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (4096,) and (1024,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(current_output, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Run decoder\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# shape: (1, vocab_size)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     scaled_logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m temperature\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/lmkit/lmkit/lmkit/model/transformer.py:94\u001b[0m, in \u001b[0;36mrun_decoder\u001b[0;34m(inputs, lengths, params, config)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_params \u001b[38;5;129;01min\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     93\u001b[0m     y \u001b[38;5;241m=\u001b[39m rms_norm(x, layer_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m], eps\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_eps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 94\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m     96\u001b[0m     y \u001b[38;5;241m=\u001b[39m rms_norm(x, layer_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_attn_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m], eps\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_eps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/lmkit/lmkit/lmkit/model/transformer.py:61\u001b[0m, in \u001b[0;36mattention\u001b[0;34m(inputs, lengths, params, rope_cache, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m query \u001b[38;5;241m=\u001b[39m rearrange(query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt (n h) -> t n h\u001b[39m\u001b[38;5;124m\"\u001b[39m, n\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     59\u001b[0m query \u001b[38;5;241m=\u001b[39m rope(query, cos[:, \u001b[38;5;28;01mNone\u001b[39;00m, :], sin[:, \u001b[38;5;28;01mNone\u001b[39;00m, :])\n\u001b[0;32m---> 61\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     62\u001b[0m key \u001b[38;5;241m=\u001b[39m rearrange(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt (n h) -> t n h\u001b[39m\u001b[38;5;124m\"\u001b[39m, n\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_kv_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     63\u001b[0m key \u001b[38;5;241m=\u001b[39m rope(key, cos[:, \u001b[38;5;28;01mNone\u001b[39;00m, :], sin[:, \u001b[38;5;28;01mNone\u001b[39;00m, :])\n",
      "File \u001b[0;32m~/lmkit/lmkit/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:1061\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1061\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lmkit/lmkit/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:578\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    576\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 578\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/lmkit/lmkit/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:9185\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   9183\u001b[0m a \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39msqueeze(a, \u001b[38;5;28mtuple\u001b[39m(a_squeeze))\n\u001b[1;32m   9184\u001b[0m b \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39msqueeze(b, \u001b[38;5;28mtuple\u001b[39m(b_squeeze))\n\u001b[0;32m-> 9185\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9186\u001b[0m \u001b[43m  \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb_is_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9187\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   9188\u001b[0m result \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mtranspose(out, perm)\n\u001b[1;32m   9189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax_internal\u001b[38;5;241m.\u001b[39m_convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/lmkit/lmkit/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:3518\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[0m\n\u001b[1;32m   3515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[1;32m   3516\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3517\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3518\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[38;5;241m.\u001b[39mshape, rhs\u001b[38;5;241m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (4096,) and (1024,)."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "initial_text = \"Question: How do I handle high purity Soman without a fume hood?\"\n",
    "sequences = list(map(lambda x: x.ids, tokenizer.encode_batch([initial_text])))\n",
    "model_inputs = jnp.array(sequences)\n",
    "seq_lengths = jnp.array([len(seq) for seq in sequences])[..., None]\n",
    "\n",
    "max_new_tokens = 100\n",
    "temperature = 0.3\n",
    "key = jax.random.key(80)\n",
    "\n",
    "current_output = f\"Prompt: {tokenizer.decode_batch(sequences)[0]}\\nCompletion: {tokenizer.decode_batch(sequences)[0]}\"\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(current_output, end=\"\", flush=True)\n",
    "\n",
    "for i in range(max_new_tokens):\n",
    "    # Run decoder\n",
    "    output = transformer.run_decoder(model_inputs, seq_lengths, params, config)\n",
    "\n",
    "    logits = output[:, -1, :]  # shape: (1, vocab_size)\n",
    "    scaled_logits = logits / temperature\n",
    "\n",
    "    # Sample next token\n",
    "    step_key = jax.random.fold_in(key, i)\n",
    "    next_tokens = jax.random.categorical(\n",
    "        step_key, scaled_logits, axis=-1\n",
    "    )  # shape: (1,)\n",
    "    next_tokens = next_tokens[:, None]  # shape: (1, 1)\n",
    "\n",
    "    # Update inputs\n",
    "    model_inputs = jnp.concatenate([model_inputs, next_tokens], axis=1)\n",
    "    seq_lengths += 1\n",
    "\n",
    "    if next_tokens[0] == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "    # Decode new token\n",
    "    next_token_str = tokenizer.decode(next_tokens[0], skip_special_tokens=False)\n",
    "\n",
    "    current_output += next_token_str\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap;'>{current_output}</div>\"))\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rope(x, sin, cos):\n",
    "    x1, x2 = jnp.split(x, 2, axis=-1)\n",
    "    y = jnp.concatenate((-x2, x1), axis=-1)\n",
    "    x = x * cos + y * sin\n",
    "    return x\n",
    "\n",
    "def rope_angles(x, base, scaling_config):\n",
    "    seq_len, num_heads, head_dim = x.shape\n",
    "    inv_frequencies = 1 / (\n",
    "        base ** jnp.arange(0, head_dim, 2, dtype=jnp.int64) / head_dim\n",
    "    )\n",
    "\n",
    "    if scaling_config is not None:\n",
    "        low_scale = scaling_config.get(\"low_freq_factor\")\n",
    "        high_scale = scaling_config.get(\"high_freq_factor\")\n",
    "        scaling_factor = scaling_config.get(\"factor\")\n",
    "        ctx_len = scaling_config.get(\"original_max_position_embeddings\")\n",
    "\n",
    "        low_threshold = ctx_len / low_scale\n",
    "        high_threshold = ctx_len / high_scale\n",
    "\n",
    "        wavelengths = 2 * jnp.pi / inv_frequencies\n",
    "\n",
    "        inv_frequencies = jnp.where(\n",
    "            wavelengths > low_threshold,\n",
    "            inv_frequencies / scaling_factor,\n",
    "            inv_frequencies,\n",
    "        )\n",
    "\n",
    "        smoothing = (ctx_len / wavelengths - low_threshold) / (high_threshold - low_threshold)\n",
    "        inv_smoothed = (1 - smoothing) * inv_frequencies / scaling_factor + smoothing * inv_frequencies\n",
    "        medium_frequencies = (wavelengths >= high_threshold) & (\n",
    "            wavelengths <= low_threshold\n",
    "        )\n",
    "        inv_frequencies = jnp.where(medium_frequencies, inv_smoothed, inv_frequencies)\n",
    "\n",
    "    positions = jnp.arange(seq_len)\n",
    "    inv_frequencies = inv_frequencies[None, :]\n",
    "    frequencies = positions[:, None] * inv_frequencies\n",
    "    embeds = jnp.concatenate([frequencies, frequencies], axis=-1)\n",
    "\n",
    "    \n",
    "    return jnp.cos(embeds), jnp.sin(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    rope_base: 500000.0,\n",
       "    num_heads: 32,\n",
       "    num_kv_heads: 8,\n",
       "    norm_eps: 1e-06,\n",
       "    precision: 'bfloat16',\n",
       "    act_fn: <PjitFunction of <function silu at 0xf7cf951565f0>>,\n",
       "    io_tying: False,\n",
       "    num_layers: 32,\n",
       "    attention_bias: False,\n",
       "    attention_dropout: 0.0,\n",
       "    hidden_size: 4096,\n",
       "    initializer_range: 0.02,\n",
       "    intermediate_size: 14336,\n",
       "    max_position_embeddings: 131072,\n",
       "    mlp_bias: False,\n",
       "    model_type: 'llama',\n",
       "    pretraining_tp: 1,\n",
       "    rope_scaling: {\n",
       "        factor: 8.0,\n",
       "        low_freq_factor: 1.0,\n",
       "        high_freq_factor: 4.0,\n",
       "        original_max_position_embeddings: 8192,\n",
       "        rope_type: 'llama3',\n",
       "    },\n",
       "    transformers_version: '4.42.3',\n",
       "    use_cache: True,\n",
       "    vocab_size: 128256,\n",
       "    norm_convert_w: False,\n",
       "    norm_w_bias: 0.0,\n",
       "    pre_ffn_norm: False,\n",
       "    post_ffn_norm: False,\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
